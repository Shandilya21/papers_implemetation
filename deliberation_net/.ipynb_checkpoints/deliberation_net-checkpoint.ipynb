{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# from data_utils import config\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_wt(lstm):\n",
    "    for name, _ in lstm.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            wt = getattr(lstm, name)\n",
    "            wt.data.uniform_(-config.rand_unif_init_mag, config.rand_unif_init_mag)\n",
    "        elif 'bias' in name:\n",
    "            # set forget bias to 1\n",
    "            bias = getattr(lstm, name)\n",
    "            n = bias.size(0)\n",
    "            start, end = n // 4, n // 2\n",
    "            bias.data.fill_(0.)\n",
    "            bias.data[start:end].fill_(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear_wt(linear):\n",
    "    linear.weight.data.normal_(std=config.trunc_norm_init_std)\n",
    "    if linear.bias is not None:\n",
    "        linear.bias.data.normal_(std=config.trunc_norm_init_std)\n",
    "\n",
    "def init_wt_normal(wt):\n",
    "    wt.data.normal_(std=config.trunc_norm_init_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(config.emb_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        init_lstm_wt(self.lstm)\n",
    "\n",
    "        self.reduce_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        init_linear_wt(self.reduce_h)\n",
    "        self.reduce_c = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        init_linear_wt(self.reduce_c)\n",
    "\n",
    "    def forward(self, x, seq_lens):\n",
    "        packed = pack_padded_sequence(x, seq_lens, batch_first=True)\n",
    "        enc_out, enc_hid = self.lstm(packed)\n",
    "        enc_out,_ = pad_packed_sequence(enc_out, batch_first=True)\n",
    "        enc_out = enc_out.contiguous()                              #bs, n_seq, 2*n_hid\n",
    "        h, c = enc_hid                                              #shape of h: 2, bs, n_hid\n",
    "        h = T.cat(list(h), dim=1)                                   #bs, 2*n_hid\n",
    "        c = T.cat(list(c), dim=1)\n",
    "        h_reduced = F.relu(self.reduce_h(h))                        #bs,n_hid\n",
    "        c_reduced = F.relu(self.reduce_c(c))\n",
    "        return enc_out, (h_reduced, c_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_attention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(encoder_attention, self).__init__()\n",
    "        self.W_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2, bias=False)\n",
    "        self.W_s = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2)\n",
    "        self.v = nn.Linear(config.hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, st_hat, h, enc_padding_mask, sum_temporal_srcs):\n",
    "        ''' Perform attention over encoder hidden states\n",
    "        :param st_hat: decoder hidden state at current time step\n",
    "        :param h: encoder hidden states\n",
    "        :param enc_padding_mask:\n",
    "        :param sum_temporal_srcs: if using intra-temporal attention, contains summation of attention weights from previous decoder time steps\n",
    "        '''\n",
    "\n",
    "        # Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)\n",
    "        et = self.W_h(h)                        # bs,n_seq,2*n_hid\n",
    "        dec_fea = self.W_s(st_hat).unsqueeze(1) # bs,1,2*n_hid\n",
    "        et = et + dec_fea\n",
    "        et = T.tanh(et)                         # bs,n_seq,2*n_hid\n",
    "        et = self.v(et).squeeze(2)              # bs,n_seq\n",
    "\n",
    "        # intra-temporal attention     (eq 3 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        if config.intra_encoder:\n",
    "            exp_et = T.exp(et)\n",
    "            if sum_temporal_srcs is None:\n",
    "                et1 = exp_et\n",
    "                sum_temporal_srcs  = get_cuda(T.FloatTensor(et.size()).fill_(1e-10)) + exp_et\n",
    "            else:\n",
    "                et1 = exp_et/sum_temporal_srcs  #bs, n_seq\n",
    "                sum_temporal_srcs = sum_temporal_srcs + exp_et\n",
    "        else:\n",
    "            et1 = F.softmax(et, dim=1)\n",
    "\n",
    "        # assign 0 probability for padded elements\n",
    "        at = et1 * enc_padding_mask\n",
    "        normalization_factor = at.sum(1, keepdim=True)\n",
    "        at = at / normalization_factor\n",
    "\n",
    "        at = at.unsqueeze(1)                    #bs,1,n_seq\n",
    "        # Compute encoder context vector\n",
    "        ct_e = T.bmm(at, h)                     #bs, 1, 2*n_hid\n",
    "        ct_e = ct_e.squeeze(1)\n",
    "        at = at.squeeze(1)\n",
    "\n",
    "        return ct_e, at, sum_temporal_srcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRNN(nn.Module):\n",
    "    def __init__(self, context_input_size, context_hidden_size, rnn_type='LSTM', \n",
    "                 num_layers=1, batch_first=True, dropout=0, bidirectional=False):\n",
    "        super(ContextRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.context_hidden_size = context_hidden_size\n",
    "        self.context_input_size = context_input_size\n",
    "        self.rnn_cell = torch_utils.rnn_cell_wrapper(rnn_type)\n",
    "        self.contextRNN = self.rnn_cell(self.context_input_size, self.context_hidden_size, num_layers=num_layers, batch_first=batch_first,\n",
    "                                            dropout=dropout, bidirectional=bidirectional)\n",
    "    def forward(self, context_enc_input):\n",
    "        context_out, context_hid = self.contextRNN(context_enc_input)\n",
    "        context_out = context_out.contiguous()\n",
    "        return context_out, context_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder_attention, self).__init__()\n",
    "        if config.intra_decoder:\n",
    "            self.W_prev = nn.Linear(config.hidden_dim, config.hidden_dim, bias=False)\n",
    "            self.W_s = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "            self.v = nn.Linear(config.hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, s_t, prev_s):\n",
    "        '''Perform intra_decoder attention\n",
    "        Args\n",
    "        :param s_t: hidden state of decoder at current time step\n",
    "        :param prev_s: If intra_decoder attention, contains list of previous decoder hidden states\n",
    "        '''\n",
    "        if config.intra_decoder is False:\n",
    "            ct_d = get_cuda(T.zeros(s_t.size()))\n",
    "        elif prev_s is None:\n",
    "            ct_d = get_cuda(T.zeros(s_t.size()))\n",
    "            prev_s = s_t.unsqueeze(1)               #bs, 1, n_hid\n",
    "        else:\n",
    "            # Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)\n",
    "            et = self.W_prev(prev_s)                # bs,t-1,n_hid\n",
    "            dec_fea = self.W_s(s_t).unsqueeze(1)    # bs,1,n_hid\n",
    "            et = et + dec_fea\n",
    "            et = T.tanh(et)                         # bs,t-1,n_hid\n",
    "            et = self.v(et).squeeze(2)              # bs,t-1\n",
    "            # intra-decoder attention     (eq 7 & 8 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            at = F.softmax(et, dim=1).unsqueeze(1)  #bs, 1, t-1\n",
    "            ct_d = T.bmm(at, prev_s).squeeze(1)     #bs, n_hid\n",
    "            prev_s = T.cat([prev_s, s_t.unsqueeze(1)], dim=1)    #bs, t, n_hid\n",
    "\n",
    "        return ct_d, prev_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNPhaseOne(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNNPhaseOne, self).__init__()\n",
    "        self.enc_attention = encoder_attention()\n",
    "        self.dec_attention = decoder_attention()\n",
    "        # self.incremental_state = None\n",
    "        self.x_context = nn.Linear(config.hidden_dim*2 + config.emb_dim, config.emb_dim) #[1024, 300]\n",
    " \n",
    "        self.first_dec = nn.LSTMCell(config.emb_dim, config.hidden_dim)\n",
    "        init_lstm_wt(self.first_dec)\n",
    "\n",
    "        self.p_gen_linear = nn.Linear(config.hidden_dim * 5 + config.emb_dim, 1)\n",
    "\n",
    "        #p_vocab\n",
    "        self.V = nn.Linear(config.hidden_dim*4, config.hidden_dim)\n",
    "        self.V1 = nn.Linear(config.hidden_dim, config.vocab_size)\n",
    "        init_linear_wt(self.V1)\n",
    "\n",
    "    def forward(self, x_t, s_t, enc_out, enc_padding_mask, ct_e, sum_temporal_srcs, prev_s):\n",
    "        x = self.x_context(T.cat([x_t, ct_e], dim=1))\n",
    "        s_t = self.first_lstm(x, s_t)\n",
    "\n",
    "        dec_h, dec_c = s_t\n",
    "        st_hat = T.cat([dec_h, dec_c], dim=1)\n",
    "        ct_e, attn_dist, sum_temporal_srcs = self.enc_attention(st_hat, enc_out, enc_padding_mask, sum_temporal_srcs)\n",
    "        ctx_d, prev_s = self.dec_attention(dec_h, prev_s)    #intra-decoder attention\n",
    "\n",
    "        p_gen = T.cat([ct_e, ct_d, st_hat, x], 1)\n",
    "        p_gen = self.p_gen_linear(p_gen)            # bs,1\n",
    "        p_gen = T.sigmoid(p_gen)                    # bs,1\n",
    "\n",
    "        out = T.cat([dec_h, ct_e, ct_d], dim=1)     # bs, 4*n_hid\n",
    "        out = self.V(out)                           # bs,n_hid\n",
    "        out = self.V1(out)                          # bs, n_vocab\n",
    "        vocab_dist = F.softmax(out, dim=1)\n",
    "        # vocab_dist = p_gen * vocab_dist\n",
    "        # attn_dist_ = (1 - p_gen) * attn_dist\n",
    "\n",
    "        # pointer mechanism (as suggested in eq 9 https://arxiv.org/pdf/1704.04368.pdf)\n",
    "        # if extra_zeros is not None:\n",
    "        #     vocab_dist = T.cat([vocab_dist, extra_zeros], dim=1)\n",
    "        # final_dist = vocab_dist.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "\n",
    "        return vocab_dist, s_t, ct_e, sum_temporal_srcs, prev_s, ctx_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNPhaseTwo(object):\n",
    "    \"\"\"DecoderRNNPhaseTwo or Deliberation Decoder\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DecoderRNNPhaseTwo, self).__init__()\n",
    "        self.dec_attention = decoder_attention()\n",
    "\n",
    "        self.x_context = nn.Linear(config.hidden_dim*2 + config.emb_dim, config.emb_dim)\n",
    "\n",
    "        self.second_dec = nn.LSTMCell(config.vocab_size, config.hidden_dim)\n",
    "        init_lstm_wt(self.second_dec)\n",
    "\n",
    "        self.p_gen_linear = nn.Linear(config.hidden_dim * 5 + config.emb_dim, 1)\n",
    "        self.S = nn.Linear(config.hidden_dim*4, config.hidden_dim)\n",
    "        self.S_p = nn.Linear(config.hidden_dim, config.vocab_size)\n",
    "        init_linear_wt(self.S_p)\n",
    "\n",
    "        \n",
    "    def forward(self, enc_out, ctx_e, ctx_d, s_t, enc_padding_mask, enc_batch_extend_vocab):\n",
    "        x_bar = self.x_context(T.cat([ctx_d, ctx_e], dim=1))\n",
    "        hid_s = self.second_lstm(x_bar, st_hat)\n",
    "\n",
    "        second_dec_h, second_dec_c = hid_s\n",
    "        s_hat = T.cat([second_dec_h, second_dec_c], dim=1)\n",
    "        ctx_phaseTwo, prev_s = self.dec_attention(second_dec_h, prev_s)\n",
    "\n",
    "        p = T.cat([enc_out, ctx_d, s_hat], dim=1)\n",
    "        p = self.p_gen_linear(p)\n",
    "        p_t = T.sigmoid(p)\n",
    "\n",
    "        out = T.cat([second_dec_h,ctx_phaseTwo], dim=1)\n",
    "        out = self.V(out)\n",
    "        out = self.V1(out)\n",
    "        output = F.softmax(out, dim=1)\n",
    "        vocab_dist = p_gen * output\n",
    "        attn_dist_ = (1 - p_t) * attn_dist\n",
    "\n",
    "        final_dist = vocab_dist.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "\n",
    "        return final_dist, s_t, ctx_phaseTwo, hid_s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        #Initialize Encoder\n",
    "        self.encoder = Encoder()\n",
    "\n",
    "        # Initialize Decoders (Along with Deliberation Decoder)\n",
    "        self.decoder_FPD = DecoderRNNPhaseOne() #First Pass Decoder\n",
    "        self.decoder_SPD = DecoderRNNPhaseTwo() #Second Pass Decoder\n",
    "        \n",
    "        self.embeds = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        init_wt_normal(self.embeds.weight)\n",
    "#         pretrained_embeddings(config.save_weight_path, self.embeds.weight)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, s_t, enc_padding_mask, persona_enc, context_size, sum_temporal_srcs, prev_s, enc_batch_extend_vocab):\n",
    "\n",
    "        assert(enc_input.size(0)==context_size) #[num_layers X batch_size X hidden_dim]\n",
    " \n",
    "        batch_size = enc_input.size(1) #[num_layers X batch_size X hidden_dim]\n",
    "        context_enc_input_in_place = Variable(T.zeros(batch_size, context_size, \\\n",
    "                                        self.context_hidden_size*2), requires_grad=True)\n",
    "\n",
    "        context_enc_input = context_enc_input_in_place.clone()\n",
    "\n",
    "        for turn in range(0,context_size):\n",
    "            \n",
    "            text_input = enc_input[turn,:] #3D to 2D (batch X seq_len)\n",
    "            enc_out, _ = self.encoder(enc_input, enc_hidden)\n",
    "\n",
    "        vocab_dist, s_t, ct_e, sum_temporal_srcs, prev_s, ctx_d = self.decoder_FPD(text_enc, s_t, enc_out, enc_padding_mask, context_enc_outputs,\\\n",
    "                                                                sum_temporal_srcs,\n",
    "                                                                prev_s)\n",
    "        final_dist, s_t, ctx_phaseTwo, hid_s  = self.decoder_SPD(enc_out, ctx_d, s_t, enc_padding_mask, enc_batch_extend_vocab)\n",
    "\n",
    "        return final_dist, s_t, ctx_phaseTwo, hid_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
